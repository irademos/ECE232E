{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the gridworld MDP class\n",
    "\n",
    "class Gridworld(object):\n",
    "    \"\"\"\n",
    "    Gridworld MDP.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size, wind, discount):\n",
    "        \"\"\"\n",
    "        grid_size: Grid size. int.\n",
    "        wind: Chance of moving randomly. float.\n",
    "        discount: MDP discount. float.\n",
    "        -> Gridworld\n",
    "        \"\"\"\n",
    "\n",
    "        self.actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = grid_size**2\n",
    "        self.grid_size = grid_size\n",
    "        self.wind = wind\n",
    "        self.discount = discount\n",
    "\n",
    "        # Preconstruct the transition probability array.\n",
    "        self.transition_probability = np.array(\n",
    "            [[[self._transition_probability(i, j, k)\n",
    "               for k in range(self.n_states)]\n",
    "              for j in range(self.n_actions)]\n",
    "             for i in range(self.n_states)])\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Gridworld({}, {}, {})\".format(self.grid_size, self.wind,\n",
    "                                              self.discount)\n",
    "    \n",
    "    def int_to_point(self, i):\n",
    "        \"\"\"\n",
    "        Convert a state int into the corresponding coordinate.\n",
    "\n",
    "        i: State int.\n",
    "        -> (x, y) int tuple.\n",
    "        \"\"\"\n",
    "\n",
    "        return (i % self.grid_size, i // self.grid_size)\n",
    "\n",
    "    def point_to_int(self, p):\n",
    "        \"\"\"\n",
    "        Convert a coordinate into the corresponding state int.\n",
    "\n",
    "        p: (x, y) tuple.\n",
    "        -> State int.\n",
    "        \"\"\"\n",
    "\n",
    "        return int(p[0] + p[1]*self.grid_size)\n",
    "\n",
    "    def neighbouring(self, i, k):\n",
    "        \"\"\"\n",
    "        Get whether two points neighbour each other. Also returns true if they\n",
    "        are the same point.\n",
    "\n",
    "        i: (x, y) int tuple.\n",
    "        k: (x, y) int tuple.\n",
    "        -> bool.\n",
    "        \"\"\"\n",
    "\n",
    "        return abs(i[0] - k[0]) + abs(i[1] - k[1]) <= 1\n",
    "\n",
    "    def _transition_probability(self, i, j, k):\n",
    "        \"\"\"\n",
    "        Get the probability of transitioning from state i to state k given\n",
    "        action j.\n",
    "\n",
    "        i: State int.\n",
    "        j: Action int.\n",
    "        k: State int.\n",
    "        -> p(s_k | s_i, a_j)\n",
    "        \"\"\"\n",
    "\n",
    "        xi, yi = self.int_to_point(i)\n",
    "        xj, yj = self.actions[j]\n",
    "        xk, yk = self.int_to_point(k)\n",
    "        \n",
    "\n",
    "        if not self.neighbouring((xi, yi), (xk, yk)):\n",
    "            return None\n",
    "\n",
    "        # Is k the intended state to move to?\n",
    "        if (xi + xj, yi + yj) == (xk, yk):\n",
    "            return None\n",
    "\n",
    "        # If these are not the same point, then we can move there by wind.\n",
    "        if (xi, yi) != (xk, yk):\n",
    "            return None\n",
    "\n",
    "        \n",
    "        # If these are the same point, we can only move here by either moving\n",
    "        # off the grid or being blown off the grid. Are we on a corner or not?\n",
    "        if on corner:\n",
    "            # Corner.\n",
    "            # Can move off the edge in two directions.\n",
    "            # Did we intend to move off the grid?\n",
    "            if we intended to move off the grid: \n",
    "                # we have the regular success chance of staying here\n",
    "                # plus an extra chance of blowing onto the *other* off-grid square\n",
    "                return None\n",
    "            else:\n",
    "                # We can blow off the grid in either direction only by wind.\n",
    "                return None\n",
    "        else:\n",
    "            # Not a corner. Is it an edge?\n",
    "            if not an edge:\n",
    "                return None\n",
    "            \n",
    "            else:#it is an edge\n",
    "                # we Can only move off the edge in one direction.\n",
    "                # Did we intend to move off the grid?\n",
    "                if We intended to move off the grid:\n",
    "                    #we have the regular success chance of staying here.\n",
    "                return None\n",
    "                else:\n",
    "                    # We can blow off the grid only by wind.\n",
    "                    return None\n",
    "\n",
    "    def reward(self, state_int):\n",
    "        \"\"\"\n",
    "        Reward for being in state state_int.\n",
    "\n",
    "        state_int: State integer. int.\n",
    "        -> Reward.\n",
    "        \"\"\"\n",
    "    \n",
    "        postive_reward = 10\n",
    "        negative_reward = -100\n",
    "        \n",
    "        #look at figure 6,7 to retrurn a reward at the given state. \n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for plotting the matrix values\n",
    "\n",
    "def plot_matrix(matrix):\n",
    "    fig, ax = plt.subplots()\n",
    "    num_rows = len(matrix)\n",
    "    min_val, max_val = 0, num_rows\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_rows):\n",
    "            c = matrix[i][j]\n",
    "            ax.text(j + 0.5, i + 0.5, '{:.1f}'.format(c), va='center', ha='center')\n",
    "\n",
    "    ax.set_xlim(min_val, max_val)\n",
    "    ax.set_ylim(max_val, min_val)\n",
    "    ax.set_xticks(np.arange(max_val))\n",
    "    ax.set_yticks(np.arange(max_val))\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the gridworld MDP with the following parameters\n",
    "\n",
    "grid_size = 10\n",
    "wind = 0.1\n",
    "discount = 0.8\n",
    "\n",
    "# Make the gridworld and associated data.\n",
    "gw = Gridworld(grid_size, wind, discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the reward value for each state of the grid\n",
    "\n",
    "def reward_grid_plot():\n",
    "    reward_matrix = np.zeros((grid_size, grid_size))\n",
    "    for j in range(grid_size):\n",
    "        for i in range(grid_size):\n",
    "            reward_matrix[i][j] = gw.reward(gw.point_to_int((i,j)))\n",
    "    plot_matrix(reward_matrix)\n",
    "    return reward_matrix\n",
    "reward_matrix = reward_grid_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For visualization generating the heat map of the ground truth reward\n",
    "\n",
    "plt.pcolor(np.flipud(reward_matrix))\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('Heat map of Reward function 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the algorithm for computing the optimal value function for each state\n",
    "## The algorithm takes as input the MDP and returns an array of optimal values,\n",
    "## where i^th value in the array corresponds to the optimal value of the i^th state.\n",
    "\n",
    "def optimal_value(n_states, n_actions, transition_probabilities, reward,\n",
    "                  discount, threshold=1e-2):\n",
    "    \"\"\"\n",
    "    Find the optimal value function.\n",
    "\n",
    "    n_states: Number of states. int.\n",
    "    n_actions: Number of actions. int.\n",
    "    transition_probabilities: Function taking (state, action, state) to\n",
    "        transition probabilities.\n",
    "    reward: Vector of rewards for each state.\n",
    "    discount: MDP discount factor. float.\n",
    "    threshold: Convergence threshold, default 1e-2. float.\n",
    "    -> Array of values for each state\n",
    "    \"\"\"\n",
    "\n",
    "    v = np.zeros(n_states)\n",
    "\n",
    "    #write code here\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the optimal values of each state in the grid\n",
    "\n",
    "# Generating the array of rewards to be passed onto the optimal value algorithm\n",
    "\n",
    "reward_states = np.zeros(gw.n_states)\n",
    "for i in range(gw.n_states):\n",
    "    reward_states[i] = gw.reward(i)\n",
    "    \n",
    "# Computing the optimal value of each state\n",
    "\n",
    "v = optimal_value(gw.n_states, gw.n_actions, gw.transition_probability, reward_states, gw.discount)\n",
    "\n",
    "# Plotting\n",
    "\n",
    "value_matrix = np.zeros((grid_size, grid_size))\n",
    "for i in range(gw.n_states):\n",
    "    value_matrix[int(i%grid_size)][int(i/grid_size)] = round(v[i], 1)\n",
    "\n",
    "plot_matrix(value_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For visualization generating the heat map of the optimal state values\n",
    "\n",
    "plt.pcolor(np.flipud(value_matrix))\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('Heat map of optimal state values for Reward function 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the function for computing the optimal policy.\n",
    "## The function takes as input the MDP and outputs a\n",
    "## deterministic policy, which is an array of actions.\n",
    "## The i^th entry in the array corresponds to the\n",
    "## optimal action to take at the i^th state.\n",
    "\n",
    "def find_policy(n_states, n_actions, transition_probabilities, reward, discount,\n",
    "                threshold=1e-2, v=None, stochastic=False):\n",
    "    \"\"\"\n",
    "    Find the optimal policy.\n",
    "\n",
    "    n_states: Number of states. int.\n",
    "    n_actions: Number of actions. int.\n",
    "    transition_probabilities: Function taking (state, action, state) to\n",
    "        transition probabilities.\n",
    "    reward: Vector of rewards for each state.\n",
    "    discount: MDP discount factor. float.\n",
    "    threshold: Convergence threshold, default 1e-2. float.\n",
    "    v: Value function (if known). Default None.\n",
    "    stochastic: Whether the policy should be stochastic. Default True.\n",
    "    -> Action probabilities for each state or action int for each state\n",
    "        (depending on stochasticity).\n",
    "    \"\"\"\n",
    "\n",
    "    if v is None:\n",
    "        v = optimal_value(n_states, n_actions, transition_probabilities, reward,\n",
    "                          discount, threshold)\n",
    "\n",
    "    def _policy(s):\n",
    "        return None\n",
    "     \n",
    "    policy = np.array([_policy(s) for s in range(n_states)])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for plotting the optimal actions at each state in the grid\n",
    "## The function takes as input the matrix containing optimal actions\n",
    "## and plots the actions for each state on the grid\n",
    "\n",
    "def plot_arrow(action_matrix):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    num_rows = len(action_matrix)\n",
    "    min_val, max_val = 0, num_rows\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_rows):\n",
    "            c = action_matrix[i][j]\n",
    "            arrow = ''\n",
    "            if(c == 0):\n",
    "                arrow = u'↓'\n",
    "            elif(c == 1):\n",
    "                arrow = u'→'\n",
    "            elif(c == 2):\n",
    "                arrow = u'↑'\n",
    "            else:\n",
    "                arrow = u'←'\n",
    "            \n",
    "            ax.text(j + 0.5, i + 0.5, arrow, va='center', ha='center')\n",
    "\n",
    "    ax.set_xlim(min_val, max_val)\n",
    "    ax.set_ylim(max_val, min_val)\n",
    "    ax.set_xticks(np.arange(max_val))\n",
    "    ax.set_yticks(np.arange(max_val))\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the optimal actions for each state in the grid\n",
    "\n",
    "# Finding the array of optimal policy\n",
    "\n",
    "optimal_policy = find_policy(gw.n_states, gw.n_actions, gw.transition_probability, reward_states, gw.discount, stochastic=False)\n",
    "\n",
    "# Generating the matrix containing the optimal actions\n",
    "\n",
    "action_matrix = np.zeros((grid_size, grid_size))\n",
    "for i in range(gw.n_states):\n",
    "    action_matrix[int(i%grid_size)][int(i/grid_size)] = optimal_policy[i]\n",
    "    \n",
    "\n",
    "# Plotting\n",
    "plot_arrow(action_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IRL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## IRL algorithm\n",
    "## LP formulation\n",
    "\n",
    "from cvxopt import matrix, solvers\n",
    "def irl(n_states, n_actions, transition_probability, policy, discount, Rmax,\n",
    "        l1):\n",
    "    \"\"\"\n",
    "    Find a reward function with inverse RL as described in Ng & Russell, 2000.\n",
    "\n",
    "    n_states: Number of states. int.\n",
    "    n_actions: Number of actions. int.\n",
    "    transition_probability: NumPy array mapping (state_i, action, state_k) to\n",
    "        the probability of transitioning from state_i to state_k under action.\n",
    "        Shape (N, A, N).\n",
    "    policy: Vector mapping state ints to action ints. Shape (N,).\n",
    "    discount: Discount factor. float.\n",
    "    Rmax: Maximum reward. float.\n",
    "    l1: l1 regularisation. float.\n",
    "    -> Reward vector\n",
    "    \"\"\"\n",
    "\n",
    "    A = set(range(n_actions))  # Set of actions to help manage reordering\n",
    "                               # actions.\n",
    "    # The transition policy convention is different here to the rest of the code\n",
    "    # for legacy reasons; here, we reorder axes to fix this. We expect the\n",
    "    # new probabilities to be of the shape (A, N, N).\n",
    "    transition_probability = np.transpose(transition_probability, (1, 0, 2))\n",
    "\n",
    "    def T(a, s):\n",
    "        \"\"\"\n",
    "        Shorthand for a dot product used a lot in the LP formulation.\n",
    "        \"\"\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    # This entire function just computes the block matrices used for the LP\n",
    "    # formulation of IRL.\n",
    "\n",
    "    # Minimise c . x.\n",
    "    \n",
    "    ##WRITE CODE HERE \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(exp_pol,ag_pol):\n",
    "    \n",
    "    num_states = len(exp_pol)\n",
    "    count = 0.0\n",
    "    \n",
    "    for i in range(num_states):\n",
    "        pass\n",
    "    \n",
    "    return np.divide(count,num_states)\n",
    "\n",
    "\n",
    "def iter_acc(grid_obj,op_pol,op_pol_compare):\n",
    "    \n",
    "    n_states = grid_obj.n_states\n",
    "    n_actions = grid_obj.n_actions\n",
    "    tr_prob = grid_obj.transition_probability\n",
    "    disc = grid_obj.discount\n",
    "    Rmax = 100\n",
    "    accuracy_array = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    lam_range = np.linspace(0,5,500)\n",
    "    \n",
    "    \n",
    "    for lam in lam_range:\n",
    "        pass\n",
    "    \n",
    "    return np.amax(accuracy_array), best_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the accuracy\n",
    "\n",
    "plt.plot(lam_range,accuracy_array)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Lambda (Reward function 2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Rmax = None\n",
    "lamda = None\n",
    "\n",
    "\n",
    "rec_reward = irl(...)\n",
    "\n",
    "# Creating the recovered reward matrix\n",
    "\n",
    "rec_reward_matrix = None\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolor(np.flipud(reward_matrix))\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('Heat map of Reward function 2')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolor(np.flipud(rec_reward_matrix))\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('Heat map of recovered reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing the optimal values of each state with extracted reward vector\n",
    "\n",
    "opt_val_rec = optimal_value(...)\n",
    "\n",
    "# Creating the recovered optimal value matrix\n",
    "\n",
    "opt_val_rec_matrix = None\n",
    "    \n",
    "\n",
    "# Generating the heatmap of the optimal values using extracted reward and the groundtruth reward\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolor(np.flipud(value_matrix))\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('Heat map of optimal values (GR)')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolor(np.flipud(opt_val_rec_matrix))\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('Heat map of optimal values (ER)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the optimal policy of the agent\n",
    "\n",
    "optimal_policy_ag = find_policy(...)\n",
    "\n",
    "# Generating the matrix containing the optimal actions for the agent\n",
    "\n",
    "action_matrix_ag = pass\n",
    "\n",
    "# Plotting\n",
    "plot_arrow(action_matrix_ag)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
