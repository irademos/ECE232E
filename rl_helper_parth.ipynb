{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"T0TmG-lA6GSD"},"outputs":[],"source":["import numpy as np\n","import numpy.random as rn\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2M2cYQC6GSF"},"outputs":[],"source":["## Define the gridworld MDP class\n","\n","class Gridworld(object):\n","    \"\"\"\n","    Gridworld MDP.\n","    \"\"\"\n","    \n","    def __init__(self, grid_size, wind, discount):\n","        \"\"\"\n","        grid_size: Grid size. int.\n","        wind: Chance of moving randomly. float.\n","        discount: MDP discount. float.\n","        -> Gridworld\n","        \"\"\"\n","\n","        self.actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n","        self.n_actions = len(self.actions)\n","        self.n_states = grid_size**2\n","        self.grid_size = grid_size\n","        self.wind = wind\n","        self.discount = discount\n","\n","        # Preconstruct the transition probability array.\n","        self.transition_probability = np.array(\n","            [[[self._transition_probability(i, j, k)\n","               for k in range(self.n_states)]\n","              for j in range(self.n_actions)]\n","             for i in range(self.n_states)])\n","\n","    def __str__(self):\n","        return \"Gridworld({}, {}, {})\".format(self.grid_size, self.wind,\n","                                              self.discount)\n","    \n","    def int_to_point(self, i):\n","        \"\"\"\n","        Convert a state int into the corresponding coordinate.\n","\n","        i: State int.\n","        -> (x, y) int tuple.\n","        \"\"\"\n","\n","        return (i % self.grid_size, i // self.grid_size)\n","\n","    def point_to_int(self, p):\n","        \"\"\"\n","        Convert a coordinate into the corresponding state int.\n","\n","        p: (x, y) tuple.\n","        -> State int.\n","        \"\"\"\n","\n","        return int(p[0] + p[1]*self.grid_size)\n","\n","    def neighbouring(self, i, k):\n","        \"\"\"\n","        Get whether two points neighbour each other. Also returns true if they\n","        are the same point.\n","\n","        i: (x, y) int tuple.\n","        k: (x, y) int tuple.\n","        -> bool.\n","        \"\"\"\n","\n","        return abs(i[0] - k[0]) + abs(i[1] - k[1]) <= 1\n","\n","    def _transition_probability(self, i, j, k):\n","        \"\"\"\n","        Get the probability of transitioning from state i to state k given\n","        action j.\n","\n","        i: State int.\n","        j: Action int.\n","        k: State int.\n","        -> p(s_k | s_i, a_j)\n","        \"\"\"\n","\n","        xi, yi = self.int_to_point(i)\n","        xj, yj = self.actions[j]\n","        xk, yk = self.int_to_point(k)\n","        \n","\n","        if not self.neighbouring((xi, yi), (xk, yk)):\n","            return None\n","\n","        # Is k the intended state to move to?\n","        if (xi + xj, yi + yj) == (xk, yk):\n","            return None\n","\n","        # If these are not the same point, then we can move there by wind.\n","        if (xi, yi) != (xk, yk):\n","            return None\n","\n","        \n","        # If these are the same point, we can only move here by either moving\n","        # off the grid or being blown off the grid. Are we on a corner or not?\n","        if on corner:\n","            # Corner.\n","            # Can move off the edge in two directions.\n","            # Did we intend to move off the grid?\n","            if we intended to move off the grid: \n","                # we have the regular success chance of staying here\n","                # plus an extra chance of blowing onto the *other* off-grid square\n","                return None\n","            else:\n","                # We can blow off the grid in either direction only by wind.\n","                return None\n","        else:\n","            # Not a corner. Is it an edge?\n","            if not an edge:\n","                return None\n","            \n","            else:#it is an edge\n","                # we Can only move off the edge in one direction.\n","                # Did we intend to move off the grid?\n","                if We intended to move off the grid:\n","                    #we have the regular success chance of staying here.\n","                return None\n","                else:\n","                    # We can blow off the grid only by wind.\n","                    return None\n","\n","    def reward(self, state_int):\n","        \"\"\"\n","        Reward for being in state state_int.\n","\n","        state_int: State integer. int.\n","        -> Reward.\n","        \"\"\"\n","    \n","        postive_reward = 10\n","        negative_reward = -100\n","        \n","        #look at figure 6,7 to retrurn a reward at the given state. \n","        \n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niufcOSH6GSI"},"outputs":[],"source":["## Function for plotting the matrix values\n","\n","def plot_matrix(matrix):\n","    fig, ax = plt.subplots()\n","    num_rows = len(matrix)\n","    min_val, max_val = 0, num_rows\n","\n","    for i in range(num_rows):\n","        for j in range(num_rows):\n","            c = matrix[i][j]\n","            ax.text(j + 0.5, i + 0.5, '{:.1f}'.format(c), va='center', ha='center')\n","\n","    ax.set_xlim(min_val, max_val)\n","    ax.set_ylim(max_val, min_val)\n","    ax.set_xticks(np.arange(max_val))\n","    ax.set_yticks(np.arange(max_val))\n","    ax.xaxis.tick_top()\n","    ax.grid()\n","    plt.show()\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3Ek_JtG6GSJ"},"outputs":[],"source":["## Creating the gridworld MDP with the following parameters\n","\n","grid_size = 10\n","wind = 0.1\n","discount = 0.8\n","\n","# Make the gridworld and associated data.\n","gw = Gridworld(grid_size, wind, discount)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFYhjFFV6GSK"},"outputs":[],"source":["## Plotting the reward value for each state of the grid\n","\n","def reward_grid_plot():\n","    reward_matrix = np.zeros((grid_size, grid_size))\n","    for j in range(grid_size):\n","        for i in range(grid_size):\n","            reward_matrix[i][j] = gw.reward(gw.point_to_int((i,j)))\n","    plot_matrix(reward_matrix)\n","    return reward_matrix\n","reward_matrix = reward_grid_plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tI5AGMD96GSL"},"outputs":[],"source":["## For visualization generating the heat map of the ground truth reward\n","\n","plt.pcolor(np.flipud(reward_matrix))\n","plt.colorbar()\n","plt.axis('off')\n","plt.title('Heat map of Reward function 1')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hAuURuuF6GSM"},"outputs":[],"source":["## Implementing the algorithm for computing the optimal value function for each state\n","## The algorithm takes as input the MDP and returns an array of optimal values,\n","## where i^th value in the array corresponds to the optimal value of the i^th state.\n","\n","def optimal_value(n_states, n_actions, transition_probabilities, reward,\n","                  discount, threshold=1e-2):\n","    \"\"\"\n","    Find the optimal value function.\n","\n","    n_states: Number of states. int.\n","    n_actions: Number of actions. int.\n","    transition_probabilities: Function taking (state, action, state) to\n","        transition probabilities.\n","    reward: Vector of rewards for each state.\n","    discount: MDP discount factor. float.\n","    threshold: Convergence threshold, default 1e-2. float.\n","    -> Array of values for each state\n","    \"\"\"\n","\n","    v = np.zeros(n_states)\n","\n","    #write code here\n","    \n","    return v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"He5aZtrR6GSN"},"outputs":[],"source":["## Plotting the optimal values of each state in the grid\n","\n","# Generating the array of rewards to be passed onto the optimal value algorithm\n","\n","reward_states = np.zeros(gw.n_states)\n","for i in range(gw.n_states):\n","    reward_states[i] = gw.reward(i)\n","    \n","# Computing the optimal value of each state\n","\n","v = optimal_value(gw.n_states, gw.n_actions, gw.transition_probability, reward_states, gw.discount)\n","\n","# Plotting\n","\n","value_matrix = np.zeros((grid_size, grid_size))\n","for i in range(gw.n_states):\n","    value_matrix[int(i%grid_size)][int(i/grid_size)] = round(v[i], 1)\n","\n","plot_matrix(value_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEAtt7a96GSO"},"outputs":[],"source":["## For visualization generating the heat map of the optimal state values\n","\n","plt.pcolor(np.flipud(value_matrix))\n","plt.colorbar()\n","plt.axis('off')\n","plt.title('Heat map of optimal state values for Reward function 1')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PgPpLQtx6GSQ"},"outputs":[],"source":["## Implementing the function for computing the optimal policy.\n","## The function takes as input the MDP and outputs a\n","## deterministic policy, which is an array of actions.\n","## The i^th entry in the array corresponds to the\n","## optimal action to take at the i^th state.\n","\n","def find_policy(n_states, n_actions, transition_probabilities, reward, discount,\n","                threshold=1e-2, v=None, stochastic=False):\n","    \"\"\"\n","    Find the optimal policy.\n","\n","    n_states: Number of states. int.\n","    n_actions: Number of actions. int.\n","    transition_probabilities: Function taking (state, action, state) to\n","        transition probabilities.\n","    reward: Vector of rewards for each state.\n","    discount: MDP discount factor. float.\n","    threshold: Convergence threshold, default 1e-2. float.\n","    v: Value function (if known). Default None.\n","    stochastic: Whether the policy should be stochastic. Default True.\n","    -> Action probabilities for each state or action int for each state\n","        (depending on stochasticity).\n","    \"\"\"\n","\n","    if v is None:\n","        v = optimal_value(n_states, n_actions, transition_probabilities, reward,\n","                          discount, threshold)\n","\n","    def _policy(s):\n","        return None\n","     \n","    policy = np.array([_policy(s) for s in range(n_states)])\n","    return policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_1PXrmd6GSS"},"outputs":[],"source":["## Function for plotting the optimal actions at each state in the grid\n","## The function takes as input the matrix containing optimal actions\n","## and plots the actions for each state on the grid\n","\n","def plot_arrow(action_matrix):\n","    \n","    fig, ax = plt.subplots()\n","    num_rows = len(action_matrix)\n","    min_val, max_val = 0, num_rows\n","\n","    for i in range(num_rows):\n","        for j in range(num_rows):\n","            c = action_matrix[i][j]\n","            arrow = ''\n","            if(c == 0):\n","                arrow = u'↓'\n","            elif(c == 1):\n","                arrow = u'→'\n","            elif(c == 2):\n","                arrow = u'↑'\n","            else:\n","                arrow = u'←'\n","            \n","            ax.text(j + 0.5, i + 0.5, arrow, va='center', ha='center')\n","\n","    ax.set_xlim(min_val, max_val)\n","    ax.set_ylim(max_val, min_val)\n","    ax.set_xticks(np.arange(max_val))\n","    ax.set_yticks(np.arange(max_val))\n","    ax.xaxis.tick_top()\n","    ax.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIyyg5R26GSU"},"outputs":[],"source":["## Plotting the optimal actions for each state in the grid\n","\n","# Finding the array of optimal policy\n","\n","optimal_policy = find_policy(gw.n_states, gw.n_actions, gw.transition_probability, reward_states, gw.discount, stochastic=False)\n","\n","# Generating the matrix containing the optimal actions\n","\n","action_matrix = np.zeros((grid_size, grid_size))\n","for i in range(gw.n_states):\n","    action_matrix[int(i%grid_size)][int(i/grid_size)] = optimal_policy[i]\n","    \n","\n","# Plotting\n","plot_arrow(action_matrix)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}